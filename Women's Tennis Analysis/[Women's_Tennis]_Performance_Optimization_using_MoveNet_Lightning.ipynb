{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpCJWA5TcK7A2M+Nl8iKs5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajraina1/datascience/blob/main/%5BWomen's_Tennis%5D_Performance_Optimization_using_MoveNet_Lightning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Women's Tennis: How does a Player's Posture Between Points affect Athletic Outcomes?"
      ],
      "metadata": {
        "id": "UAN0x0NaZufO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#OVERVIEW\n",
        "\n",
        "Existing empirical research in athletic posture analysis either deals with a player's posture *during* points (i.e. stroke-mechanics) ; or , with a player's posture immediately following victory.\n",
        "\n",
        "Using **women's tennis** as a case study, this research **examines players' posture *between* points**, throughout the match, as a potentially **distinguishing factor of skill** . We employ MoveNet AI's Deeplearning Pose Estimation Rendering to empirically answer the following question.\n",
        "\n",
        "> QUESTION: Do **top-ranked** WTA tennis players exhibit **stronger posturing between points**, compared to **lower-ranked** Women's Challenger tennis players?\n",
        "\n",
        "An affirmative response to this question would suggest that in addition to stroke mechanics, coaches and players should place special emphasis on **between-point posture** as a driver of **long-term athletic success**.\n",
        "\n",
        "![text](https://drive.google.com/uc?export=view&id=1VE4IRCMfL3bcCrcQF7CG9L3gYI95efQn)\n",
        "\n",
        "To empirically answer this question, we look at **equally-skilled matches** and employ the following methodology:\n",
        "\n",
        "> 1) Take a simple random sample of $X=30$ images from WTA & Challengers atheletes' posture between points (*),\n",
        "\n",
        "> 2) Use MoveNet AI & tensorflow to systematically classify each posture as negative, neutral, or positive.\n",
        "\n",
        "Because the postures are **drawn at random** and from equally-skilled matches, the posture would not be related to one player completely dominating (or being dominated by) another. We then use our **systematic posture classification** to examine whether there is an empirical difference between top-ranked WTA players and lower ranked women's Challenger players.  \n",
        "\n",
        "This research is the **first to examine** whether *between-point* posture differentiates atheletes, instead of traditional research which focuses on *during-point* stroke analysis or *post-game* posture analysis. Please see the **full Python code**, which includes **MoveNet rendering** & statistical analysis tools, at [Raj's Google Colab Doc](https://colab.research.google.com/drive/1sBl3iQHMe2RDq8ErY4aDI-R7S2rgHWbR?usp=sharing).\n",
        "\n",
        " (*) Note that due to time and resource restraints, the chosen value of $X$ is relatively small. This analysis is not airtight, but is intended as a proof-of-concept to show the researcher's facility with classical pose-estimation tools and ability to formulate an interesting & relevant problem, and to execute on a reasonable methodology.\n"
      ],
      "metadata": {
        "id": "3vKHOLAbfPbU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATA COLLECTION\n",
        "\n",
        "We scrape $X=30$ images from various female tennis matches. Images are drawn as in a simple random sample, selected **uniformly and at random** across the duration of the match, subject to the following restrictions:\n",
        "\n",
        "*   15 images are taken from top-ranked WTA players in equally-skilled matches (**Serena Williams** in Serena VS Venus matches, **Aryna Sabalenka** in Sabalenka VS Swiatek matches, and **Maria Sharapova** in Sharapova VS Wozniaki matches)\n",
        "*   15 images are taken from lower-ranked Challengers female players in equally-skilled matches (**Priscilla Hon** in Hon VS Birell matches, **Eva Lys** in Lys VS Vondrousova matches, **Lou Brouleau** in Brouleau VS Thamchaiwat matches)\n",
        "* All images are drawn from when the players are *between* points.\n",
        "\n",
        "\n",
        "We then process each image using MoveNet Lightning's Deeplearning Pose Estimation Rendering tool to rigorously map limb location and relative distances. We use the resulting pose estimation to gather an empirical \"pose-value\" , which maps to a classification of each posture as negative, neutral, or positive based on our own empirical classification algorithm (detailed in the next section).\n",
        "\n",
        "Hence, each of the 30 images is reduced to a three-dimensional **(string, decimal, string)** vector which captures **(player, empirical pose value, pose classification)**. For example, sample data may look like:\n",
        "\n",
        "**(Serena Williams, 0.453, negative pose)**\n",
        "\n",
        "**(Aryna Sabalenka, 0.321, positive pose)**\n",
        "\n",
        "etc.\n",
        "\n",
        "Finally, we use a **t-test on summary statistics** to evaluate whether there is a posture difference between the top-ranked WTA players and the lower-ranked WTA players.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8Dh-QVpuZKmk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Empirical Classification of Posture\n",
        "\n",
        "Classical and **well established research** suggests the following:\n",
        "\n",
        "1.   A common **power/victory pose** is captured by an **\"arms-above-the-head\"** posture\n",
        "2.   Striking a particular pose **affects a person's mindset**, regardless of whether the person is striking the pose conciously, or even if they are forcing themselves into the posture.\n",
        "\n",
        "\n",
        "![text](https://drive.google.com/uc?export=view&id=1BW7GV6sYdhjGLDHhirswfZOQoxMOZz6X)\n",
        "\n",
        "\n",
        "Hence, Point 1) suggests that we can estimate the \"positivity\" of a posture by examining the **distance** between the **players' hands and her head**. Point 2) implies that the **closer** the hands are to the head, the **more \"positive\"** the player will feel, whether or not the action is concious, or **even if it is forced**. The exception to this rule is if the hand is directly on the face, indicating hesitation (negative posture), or if the hands are above the head in exasperation (negative posture). We handle these cases separately.\n",
        "\n",
        "But, if one player is shorter than another, the shorter player's hands will be closer to her head by default, even at a completely neutral posture. Hence, we **normalize** by overall **player height** to account for this.\n",
        "\n",
        "Specifically, we use the following **empirical formula** to assign the following empirical pose value to each players' posture:\n",
        "\n",
        "$\\text{empirical pose value} = \\frac{\\text{(right eye height - right wrist height) + (left eye height - left wrist height)}}{\\text{(right eye height - right ankle height) + (left eye height - left ankle height)}}$\n",
        "\n",
        "Each of these keypoints is captured by MoveNet's posture recognition, detailed [here](https://tfhub.dev/google/movenet/singlepose/lightning/4).\n",
        "\n",
        "Note that **smaller** empirical pose values imply a **more positive posture**. Indeed, as the hands approach the head, the numerator gets smaller and the pose gets more positive; if the hands were above the head entirely (the strongest posture), then the numerator would be negative and the empirical pose value would be negative overall. Smaller empirical pose values also can result from a large denominator, which means that the eyes and ankles are far apart and the player is erect (as opposed to hunched) in stature. All of this indicates that **smaller empirical pose values** imply a **more positive posture**, and vice versa.\n",
        "\n",
        "Indeed, the postive-posturing of Serena Williams directly above recieved a lower value of -0.0634 when ran through our MoveNet algorithm, whereas the negative posturing below received a higher value of 0.4783, **both as expected** based on qualitative assessment.\n",
        "\n",
        "![text](https://drive.google.com/uc?export=view&id=1wom9WmcuyPXI4sDkwnnArDoEHOy-vOwY)\n"
      ],
      "metadata": {
        "id": "ZOW1W7swZ4P_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Results and Analysis\n",
        "\n",
        "To restate the initial question more generally, we examine ...\n",
        "\n",
        "> How do the **between-point postures** of **top-ranked** WTA tennis players differ from the between-point **postures of lower-ranked** Women's Challenger tennis players?\n",
        "\n",
        "As detailed in the DATA COLLECTION section, we scraped 30 images: 15 from top-ranked WTA tennis players, and 15 from lower-ranked players. Each image was taken randomly, in between points, and from equally-skilled matches. We then used MoveNet Lightning to determine an empirical pose value and classify it as detailed in the EMPIRICAL CLASSIFICATION section. **All images and analysis** can be found on [Raj's Google Colab Doc](https://colab.research.google.com/drive/1sBl3iQHMe2RDq8ErY4aDI-R7S2rgHWbR?usp=sharing) and the [Player Posture Image Reference Sheet ](https://docs.google.com/spreadsheets/d/1XYoEnYKDukmcAHkx7W9bBJ-sOEzLDQBBNTHWQkx5Iz8/edit#gid=0).\n",
        "\n",
        "Resulting summary statistics are outlined here:"
      ],
      "metadata": {
        "id": "fQ9GPgjErAOn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following suggests that **WTA players** do, in fact, **exhibit a higher proportion of positive poses** than Challenger-level players (**40%** compared to **27%** of poses classified as positive)."
      ],
      "metadata": {
        "id": "wOiBw3CuS4ka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Proportion of  poses , by type\n",
        "df['Proportion'] = pd.qcut(df['Score'], 3, labels=[\"Positive Poses\", \"Neutral Poses\", \"Negative Poses\"])\n",
        "print(df.groupby(['Type','Proportion']).count()['Score']/15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1aWZsE8M4fj",
        "outputId": "f26f8657-ff5e-4d1a-89e5-cff9bf01a513"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type        Proportion    \n",
            "Challenger  Positive Poses    0.266667\n",
            "            Neutral Poses     0.333333\n",
            "            Negative Poses    0.400000\n",
            "WTA         Positive Poses    0.400000\n",
            "            Neutral Poses     0.333333\n",
            "            Negative Poses    0.266667\n",
            "Name: Score, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "Indeed, even when **averaging across the empirical_pose_values** for both groups, **WTA plays show a slight edge** in more positive posturing (**3.9%** uplift).\n",
        "\n"
      ],
      "metadata": {
        "id": "i8d7i5U6TMC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Avg score by type\n",
        "print(\" Avg Score by Type\")\n",
        "print(df.groupby(['Type'])['Score'].mean())\n",
        "print(\"\\n StDev Score by Type\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6udCSSbNo5R",
        "outputId": "ffc4e34d-e65c-4b76-d45c-48bc3828308a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Avg Score by Type\n",
            "Type\n",
            "Challenger    0.398018\n",
            "WTA           0.383617\n",
            "Name: Score, dtype: float64\n",
            "\n",
            " StDev Score by Type\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also include **each individual player's** mean and stdev **posture score**. Further, there appears to be **no clear separation**, however, between **WTA and Challengers** in the distributional plot of pose_values, shown below."
      ],
      "metadata": {
        "id": "-iwefYLpvWT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Avg score by player\n",
        "print(df.groupby(['Type'])['Score'].std())\n",
        "print(\"\\n Avg Score by Player\")\n",
        "print(df.groupby(['Player'])['Score'].mean())\n",
        "print(\"\\n StDev Score by Player\")\n",
        "print(df.groupby(['Player'])['Score'].std())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGTGwZf0vd7H",
        "outputId": "118cf4f0-b0fd-455d-ac85-86a4f4cfb41e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type\n",
            "Challenger    0.075714\n",
            "WTA           0.063723\n",
            "Name: Score, dtype: float64\n",
            "\n",
            " Avg Score by Player\n",
            "Player\n",
            "Aryna Sabalenka    0.386870\n",
            "Eva Lys            0.354640\n",
            "Lou Brouleau       0.440520\n",
            "Maria Sharapova    0.370434\n",
            "Priscilla Hon      0.398895\n",
            "Serena Williams    0.393548\n",
            "Name: Score, dtype: float64\n",
            "\n",
            " StDev Score by Player\n",
            "Player\n",
            "Aryna Sabalenka    0.054136\n",
            "Eva Lys            0.071674\n",
            "Lou Brouleau       0.048025\n",
            "Maria Sharapova    0.072154\n",
            "Priscilla Hon      0.089495\n",
            "Serena Williams    0.075642\n",
            "Name: Score, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Hist by type\n",
        "\n",
        "WTA = df[df['Type'] == 'WTA']['Score']\n",
        "Challenger = df[df['Type'] == 'Challenger']['Score']\n",
        "\n",
        "bins = np.linspace(0, 1, 100)\n",
        "\n",
        "plt.hist(WTA, bins, alpha=0.5, label='WTA')\n",
        "plt.hist(Challenger, bins, alpha=0.5, label='Challenger')\n",
        "plt.legend(loc='upper right')\n",
        "plt.xlabel('Empirical Pose Value')\n",
        "plt.ylabel('# Poses')\n",
        "plt.title('Pose Values for WTA VS. Challenger')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "id": "lpBXqTACN1i8",
        "outputId": "0e69c713-9e04-4934-c441-027130ddcb27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Pose Values for WTA VS. Challenger')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indeed, a **small effect size** coupled with a **low p-value** makes us ***unable to reject the null hypothesis*** of \"no difference between the two groups\"."
      ],
      "metadata": {
        "id": "oMGSG1PdXWYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#t-test by type\n",
        "tstat, pval = scipy.stats.ttest_ind(df[df['Type'] == 'WTA']['Score'], df[df['Type'] == 'Challenger']['Score'])\n",
        "\n",
        "print(\"tstat: \" , tstat)\n",
        "print( \"pval: \" , pval)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzfUwyUwN87x",
        "outputId": "6406b076-3176-45ee-fa97-cbc2857a6a8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tstat:  -0.5636058490926592\n",
            "pval:  0.5775088107473891\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusions, Limitations, and Future Work\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "The results of this study suggest that while higher-ranked **WTA players slightly edge out** lower-ranked Challengers in posturing, the low sample size, **small effect size**, and **high p-value** ultimately suggest that **more research is needed** before affirming that higher ranked female tennis players indeed exhibit better posture, on average, than lower ranked female tennis players.\n",
        "\n",
        "In fact, players between both groups (WTA and Challenger) **exhibited many commonalties in posturing**: clear negative posturing when a match loss was imminent; clear neutral posturing when down big on their own serve; no  incongruity between the point outcome and the posturing, and so on.\n",
        "\n",
        "Moreover, **each player demonstrated her own distinct pattern of posturing**, and better WTA players appeared to be **no less prone** to negative posturing than did lower-ranked Challenger players. Conversely, lower-ranked players appeared to **exhibit the same pattern of positive posturing** as did the higher-ranked players after winning a point, thus making the conclusion that better players exhibit better posture between points likely **untenable without more research**.\n",
        "\n",
        "We list some qualitative analysis below:\n",
        "\n",
        "--------\n",
        "\n",
        "**Williams** displays strong negative posture after a lost point, and strong positive postures after a won point. Her postures are often more complex — kneeling on the floor, touching the face and screaming, leg kicking, etc.\n",
        "\n",
        "**Sabalenka** seemed to be more (negatively) emotive with face and eyes, and generally pretty reserved with body postures whether she won or lost the point. Indeed, Sabalenka’s posture scores averaged around the same as Williams’,  but with less variance.\n",
        "\n",
        "**Sharapova** displayed hardly any signs of negative body posture after a lost point, and only slight positive postures after winning points.\n",
        "\n",
        "**Hon** was very reserved with her posture after either a won or a lost point. Hon hardly showed positive displays after a won point, and showed very slight negative displays after a lost point. She hung her head frequently, typically an indication of a negative posture, though this was likely done more in thought than out of despair. As a result, her  empirical_pose_values were higher (indicating worse posture, from her head-hanging) though a human interpreter may have reached different conclusions than MoveNet.\n",
        "\n",
        "**Lys** was the unique player who showed consistent positive posturing after a win. She typically did this in the form of a fist pump. Conversely, she showed hardly any negative displays after a loss.\n",
        "\n",
        "**Brouleau** in most cases tended towards neutral to negative posturing, with relatively little variance.\n",
        "\n",
        "-------\n",
        "\n",
        "The **considerable emotive and posture variation** amongst players — and especially the **appearance of an outlier like Serena Williams**, who displays strong and clear negative posture despite being far and away the best of the selected players —  indicates that positive posture may not be directly correlated with better players.  Instead, it seems that **each player has a distinctive mode of expression**, which can vary considerably from player to player and seems to not be superior or even differentiable when comparing  WTA players to lower-ranked Challengers.\n",
        "\n",
        "Maintaining a positive posture between-points, then, **likely is far out-dominated by stroke mechanics, hitting power, “clutchness”, and other factors**. This is not to discount the importance of posturing in affecting mental belief, and the importance of mental belief in winning tennis matches, as **both seem to be intuitively stressed by coaches and professionals**. However, in the case of tennis, **intensity of the posture seems more dictated by the intensity of the match**, with more variability in posturing arising from a more engaged crowd during more important moments of the match.\n",
        "\n",
        "**Limitations and Future Work**\n",
        "\n",
        "Though the questions posed by the study are unique, there are limitations to the methodology due to resource restraints. Here, we list a few.\n",
        "\n",
        "First, the **classification of each posture** as negative/neutral/positive, though rooted in empirical literature, **can be improved upon**. For example, there is a 15-second window between points, from which the player may go through a range of postures. Therefore, it would be more accurate to take an of **\"average-posture\"** of the **between-point window**, by using a **full video** instead of a **collection of still images**. Additionally, we must be careful to acknowledge that match-intensity  (e.g. Wimbledon finals VS an ITF Challengers final) may itself be a driver of certain modes of posture. **Within-group analysis of variance** with a larger sample size could **account for match-intensity**.\n",
        "\n",
        "Similarly, we would use more sophisticated methods of pose estimation **(spine angle, neck/head relationship)**, also in conjunction with **facial recognition**, to obtain a more reliable prediction of the pose. We note that MoveNet's pose estimation is not perfect, and also that there are certain **nuances to poses** that may not be captured entirely by a strict equational output. Hanging head could be indication of thinking, and was done in most cases. Tilting the head back in some cases can be a sign of positive posture, but in most cases during the matches indicated the player was frustrated with the outcome. A lifted racket could indicate an apology, instead of a victory-pose, and so on.\n",
        "\n",
        "\n",
        "\n",
        "Finally, we could examine another intriguing question:\n",
        "\n",
        "> QUESTION 2 : Does a WTA athlete's **posture *between* points** affect her success on ***subsequent*** points?\n",
        "\n",
        " This would require **much more extensive data collection** and normalization. In this case, a regression that analyzes which explanatory variable amongst several (one of which is \"positive posture\") most strongly predicts a \"victory/loss\" response variable seems adequate. We could similarly examine the relative **relationship between posture and mechanics** in determining athletic success.\n",
        "\n",
        "Note that having a sufficiently large value $X$ of still images, sourced from several dozen or hundred different female players, would normalize for the first noted limitation, though again, due to resource restraints we limited the value of $X$ to a reasonably acheivable value. The intent of this paper was not to form an airtight conclusion, but rather to demonstrate the researcher's facility with classical pose-estimation tools and ability to formulate and execute on a reasonable methodology.\n",
        "\n",
        "**Thank you for reading!**\n",
        "\n"
      ],
      "metadata": {
        "id": "iPt1pZdvTxo5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#APPENDIX: CODE"
      ],
      "metadata": {
        "id": "bLURMyh9T_sv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install & Load Dependencies"
      ],
      "metadata": {
        "id": "VM07keVnU5OP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib\n",
        "!pip install tensorflow==2.8.0\n",
        "!pip install opencv-python\n",
        "!pip install -q imageio\n",
        "!pip install -q opencv-python\n",
        "!pip install -q git+https://github.com/tensorflow/docs\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow_docs.vis import embed\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import math\n",
        "import scipy\n",
        "\n",
        "\n",
        "\n",
        "# Import matplotlib libraries\n",
        "from scipy import stats\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.collections import LineCollection\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# Some modules to display an animation using imageio.\n",
        "import imageio\n",
        "from IPython.display import HTML, display\n",
        "from keras.preprocessing.image import ImageDataGenerator\n"
      ],
      "metadata": {
        "id": "7CournPcUMOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Models"
      ],
      "metadata": {
        "id": "f2gggq2uUzxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"movenet_lightning\"\n",
        "\n",
        "if \"tflite\" in model_name:\n",
        "  if \"movenet_lightning_f16\" in model_name:\n",
        "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/float16/4?lite-format=tflite\n",
        "    input_size = 192\n",
        "  elif \"movenet_thunder_f16\" in model_name:\n",
        "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/float16/4?lite-format=tflite\n",
        "    input_size = 256\n",
        "  elif \"movenet_lightning_int8\" in model_name:\n",
        "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/int8/4?lite-format=tflite\n",
        "    input_size = 192\n",
        "  elif \"movenet_thunder_int8\" in model_name:\n",
        "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/int8/4?lite-format=tflite\n",
        "    input_size = 256\n",
        "  else:\n",
        "    raise ValueError(\"Unsupported model name: %s\" % model_name)\n",
        "\n",
        "  # Initialize the TFLite interpreter\n",
        "  interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
        "  interpreter.allocate_tensors()\n",
        "\n",
        "  def movenet(input_image):\n",
        "    \"\"\"Runs detection on an input image.\n",
        "\n",
        "    Args:\n",
        "      input_image: A [1, height, width, 3] tensor represents the input image\n",
        "        pixels. Note that the height/width should already be resized and match the\n",
        "        expected input resolution of the model before passing into this function.\n",
        "\n",
        "    Returns:\n",
        "      A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
        "      coordinates and scores.\n",
        "    \"\"\"\n",
        "    # TF Lite format expects tensor type of uint8.\n",
        "    input_image = tf.cast(input_image, dtype=tf.uint8)\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "    interpreter.set_tensor(input_details[0]['index'], input_image.numpy())\n",
        "    # Invoke inference.\n",
        "    interpreter.invoke()\n",
        "    # Get the model prediction.\n",
        "    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
        "    return keypoints_with_scores\n",
        "\n",
        "else:\n",
        "  if \"movenet_lightning\" in model_name:\n",
        "    module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
        "    input_size = 192\n",
        "  elif \"movenet_thunder\" in model_name:\n",
        "    module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/thunder/4\")\n",
        "    input_size = 256\n",
        "  else:\n",
        "    raise ValueError(\"Unsupported model name: %s\" % model_name)\n",
        "\n",
        "  def movenet(input_image):\n",
        "    \"\"\"Runs detection on an input image.\n",
        "\n",
        "    Args:\n",
        "      input_image: A [1, height, width, 3] tensor represents the input image\n",
        "        pixels. Note that the height/width should already be resized and match the\n",
        "        expected input resolution of the model before passing into this function.\n",
        "\n",
        "    Returns:\n",
        "      A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
        "      coordinates and scores.\n",
        "    \"\"\"\n",
        "    model = module.signatures['serving_default']\n",
        "\n",
        "    # SavedModel format expects tensor type of int32.\n",
        "    input_image = tf.cast(input_image, dtype=tf.int32)\n",
        "    # Run model inference.\n",
        "    outputs = model(input_image)\n",
        "    # Output is a [1, 1, 17, 3] tensor.\n",
        "    keypoints_with_scores = outputs['output_0'].numpy()\n",
        "    return keypoints_with_scores"
      ],
      "metadata": {
        "id": "G6_0sWy0Uo5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Viz Functions\n"
      ],
      "metadata": {
        "id": "i7Ju2Mstbdtb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary that maps from joint names to keypoint indices.\n",
        "KEYPOINT_DICT = {\n",
        "    'nose': 0,\n",
        "    'left_eye': 1,\n",
        "    'right_eye': 2,\n",
        "    'left_ear': 3,\n",
        "    'right_ear': 4,\n",
        "    'left_shoulder': 5,\n",
        "    'right_shoulder': 6,\n",
        "    'left_elbow': 7,\n",
        "    'right_elbow': 8,\n",
        "    'left_wrist': 9,\n",
        "    'right_wrist': 10,\n",
        "    'left_hip': 11,\n",
        "    'right_hip': 12,\n",
        "    'left_knee': 13,\n",
        "    'right_knee': 14,\n",
        "    'left_ankle': 15,\n",
        "    'right_ankle': 16\n",
        "}\n",
        "\n",
        "# Maps bones to a matplotlib color name.\n",
        "KEYPOINT_EDGE_INDS_TO_COLOR = {\n",
        "    (0, 1): 'm',\n",
        "    (0, 2): 'c',\n",
        "    (1, 3): 'm',\n",
        "    (2, 4): 'c',\n",
        "    (0, 5): 'm',\n",
        "    (0, 6): 'c',\n",
        "    (5, 7): 'm',\n",
        "    (7, 9): 'm',\n",
        "    (6, 8): 'c',\n",
        "    (8, 10): 'c',\n",
        "    (5, 6): 'y',\n",
        "    (5, 11): 'm',\n",
        "    (6, 12): 'c',\n",
        "    (11, 12): 'y',\n",
        "    (11, 13): 'm',\n",
        "    (13, 15): 'm',\n",
        "    (12, 14): 'c',\n",
        "    (14, 16): 'c'\n",
        "}\n",
        "\n",
        "def _keypoints_and_edges_for_display(keypoints_with_scores,\n",
        "                                     height,\n",
        "                                     width,\n",
        "                                     keypoint_threshold=0.11):\n",
        "  \"\"\"Returns high confidence keypoints and edges for visualization.\n",
        "\n",
        "  Args:\n",
        "    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
        "      the keypoint coordinates and scores returned from the MoveNet model.\n",
        "    height: height of the image in pixels.\n",
        "    width: width of the image in pixels.\n",
        "    keypoint_threshold: minimum confidence score for a keypoint to be\n",
        "      visualized.\n",
        "\n",
        "  Returns:\n",
        "    A (keypoints_xy, edges_xy, edge_colors) containing:\n",
        "      * the coordinates of all keypoints of all detected entities;\n",
        "      * the coordinates of all skeleton edges of all detected entities;\n",
        "      * the colors in which the edges should be plotted.\n",
        "  \"\"\"\n",
        "  keypoints_all = []\n",
        "  keypoint_edges_all = []\n",
        "  edge_colors = []\n",
        "  num_instances, _, _, _ = keypoints_with_scores.shape\n",
        "  for idx in range(num_instances):\n",
        "    kpts_x = keypoints_with_scores[0, idx, :, 1]\n",
        "    kpts_y = keypoints_with_scores[0, idx, :, 0]\n",
        "    kpts_scores = keypoints_with_scores[0, idx, :, 2]\n",
        "    kpts_absolute_xy = np.stack(\n",
        "        [width * np.array(kpts_x), height * np.array(kpts_y)], axis=-1)\n",
        "    kpts_above_thresh_absolute = kpts_absolute_xy[\n",
        "        kpts_scores > keypoint_threshold, :]\n",
        "    keypoints_all.append(kpts_above_thresh_absolute)\n",
        "\n",
        "    for edge_pair, color in KEYPOINT_EDGE_INDS_TO_COLOR.items():\n",
        "      if (kpts_scores[edge_pair[0]] > keypoint_threshold and\n",
        "          kpts_scores[edge_pair[1]] > keypoint_threshold):\n",
        "        x_start = kpts_absolute_xy[edge_pair[0], 0]\n",
        "        y_start = kpts_absolute_xy[edge_pair[0], 1]\n",
        "        x_end = kpts_absolute_xy[edge_pair[1], 0]\n",
        "        y_end = kpts_absolute_xy[edge_pair[1], 1]\n",
        "        line_seg = np.array([[x_start, y_start], [x_end, y_end]])\n",
        "        keypoint_edges_all.append(line_seg)\n",
        "        edge_colors.append(color)\n",
        "  if keypoints_all:\n",
        "    keypoints_xy = np.concatenate(keypoints_all, axis=0)\n",
        "  else:\n",
        "    keypoints_xy = np.zeros((0, 17, 2))\n",
        "\n",
        "  if keypoint_edges_all:\n",
        "    edges_xy = np.stack(keypoint_edges_all, axis=0)\n",
        "  else:\n",
        "    edges_xy = np.zeros((0, 2, 2))\n",
        "  return keypoints_xy, edges_xy, edge_colors\n",
        "\n",
        "\n",
        "def draw_prediction_on_image(\n",
        "    image, keypoints_with_scores, crop_region=None, close_figure=False,\n",
        "    output_image_height=None):\n",
        "  \"\"\"Draws the keypoint predictions on image.\n",
        "\n",
        "  Args:\n",
        "    image: A numpy array with shape [height, width, channel] representing the\n",
        "      pixel values of the input image.\n",
        "    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
        "      the keypoint coordinates and scores returned from the MoveNet model.\n",
        "    crop_region: A dictionary that defines the coordinates of the bounding box\n",
        "      of the crop region in normalized coordinates (see the init_crop_region\n",
        "      function below for more detail). If provided, this function will also\n",
        "      draw the bounding box on the image.\n",
        "    output_image_height: An integer indicating the height of the output image.\n",
        "      Note that the image aspect ratio will be the same as the input image.\n",
        "\n",
        "  Returns:\n",
        "    A numpy array with shape [out_height, out_width, channel] representing the\n",
        "    image overlaid with keypoint predictions.\n",
        "  \"\"\"\n",
        "  height, width, channel = image.shape\n",
        "  aspect_ratio = float(width) / height\n",
        "  fig, ax = plt.subplots(figsize=(12 * aspect_ratio, 12))\n",
        "  # To remove the huge white borders\n",
        "  fig.tight_layout(pad=0)\n",
        "  ax.margins(0)\n",
        "  ax.set_yticklabels([])\n",
        "  ax.set_xticklabels([])\n",
        "  plt.axis('off')\n",
        "\n",
        "  im = ax.imshow(image)\n",
        "  line_segments = LineCollection([], linewidths=(4), linestyle='solid')\n",
        "  ax.add_collection(line_segments)\n",
        "  # Turn off tick labels\n",
        "  scat = ax.scatter([], [], s=60, color='#FF1493', zorder=3)\n",
        "\n",
        "  (keypoint_locs, keypoint_edges,\n",
        "   edge_colors) = _keypoints_and_edges_for_display(\n",
        "       keypoints_with_scores, height, width)\n",
        "\n",
        "  line_segments.set_segments(keypoint_edges)\n",
        "  line_segments.set_color(edge_colors)\n",
        "  if keypoint_edges.shape[0]:\n",
        "    line_segments.set_segments(keypoint_edges)\n",
        "    line_segments.set_color(edge_colors)\n",
        "  if keypoint_locs.shape[0]:\n",
        "    scat.set_offsets(keypoint_locs)\n",
        "\n",
        "  if crop_region is not None:\n",
        "    xmin = max(crop_region['x_min'] * width, 0.0)\n",
        "    ymin = max(crop_region['y_min'] * height, 0.0)\n",
        "    rec_width = min(crop_region['x_max'], 0.99) * width - xmin\n",
        "    rec_height = min(crop_region['y_max'], 0.99) * height - ymin\n",
        "    rect = patches.Rectangle(\n",
        "        (xmin,ymin),rec_width,rec_height,\n",
        "        linewidth=1,edgecolor='b',facecolor='none')\n",
        "    ax.add_patch(rect)\n",
        "\n",
        "  fig.canvas.draw()\n",
        "  image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
        "  image_from_plot = image_from_plot.reshape(\n",
        "      fig.canvas.get_width_height()[::-1] + (3,))\n",
        "  plt.close(fig)\n",
        "  if output_image_height is not None:\n",
        "    output_image_width = int(output_image_height / height * width)\n",
        "    image_from_plot = cv2.resize(\n",
        "        image_from_plot, dsize=(output_image_width, output_image_height),\n",
        "         interpolation=cv2.INTER_CUBIC)\n",
        "  return image_from_plot\n",
        "\n",
        "def to_gif(images, duration):\n",
        "  \"\"\"Converts image sequence (4D numpy array) to gif.\"\"\"\n",
        "  imageio.mimsave('./animation.gif', images, duration=duration)\n",
        "  return embed.embed_file('./animation.gif')\n",
        "\n",
        "def progress(value, max=100):\n",
        "  return HTML(\"\"\"\n",
        "      <progress\n",
        "          value='{value}'\n",
        "          max='{max}',\n",
        "          style='width: 100%'\n",
        "      >\n",
        "          {value}\n",
        "      </progress>\n",
        "  \"\"\".format(value=value, max=max))"
      ],
      "metadata": {
        "id": "w4KXzBjybiYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Processing & Inference of Each Image"
      ],
      "metadata": {
        "id": "1f8qBnavWhr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -o input_image.jpeg https://rajraina.com/wp-content/uploads/2023/06/serena-win.jpeg --silent\n",
        "\n",
        "# Load the input image.\n",
        "image_path = 'input_image.jpeg'\n",
        "image = tf.io.read_file(image_path)\n",
        "image = tf.image.decode_jpeg(image)\n",
        "\n",
        "#Inference\n",
        "# Resize and pad the image to keep the aspect ratio and fit the expected size.\n",
        "input_image = tf.expand_dims(image, axis=0)\n",
        "input_image = tf.image.resize_with_pad(input_image, input_size, input_size)\n",
        "\n",
        "# Run model inference.\n",
        "keypoints_with_scores = movenet(input_image)\n",
        "\n",
        "# Visualize the predictions with image.\n",
        "display_image = tf.expand_dims(image, axis=0)\n",
        "display_image = tf.cast(tf.image.resize_with_pad(\n",
        "    display_image, 1280, 1280), dtype=tf.int32)\n",
        "output_overlay = draw_prediction_on_image(np.squeeze(display_image.numpy(), axis=0), keypoints_with_scores)\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.imshow(output_overlay)\n",
        "_ = plt.axis('off')\n",
        "\n",
        "#Compute the empirical_pose_value; only the z-coordinate is necessary\n",
        "this_empirical_pose_value = ((keypoints_with_scores[0][0][2][0]-keypoints_with_scores[0][0][10][0])+(keypoints_with_scores[0][0][1][0]-keypoints_with_scores[0][0][9][0]))/ \\\n",
        "((keypoints_with_scores[0][0][2][0]-keypoints_with_scores[0][0][16][0])+(keypoints_with_scores[0][0][1][0]-keypoints_with_scores[0][0][15][0]))\n",
        "\n",
        "print(this_empirical_pose_value)\n"
      ],
      "metadata": {
        "id": "wgDKwS3MWjhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sheet_url = \"https://docs.google.com/spreadsheets/d/1XYoEnYKDukmcAHkx7W9bBJ-sOEzLDQBBNTHWQkx5Iz8/edit#gid=0\"\n",
        "url_1 = sheet_url.replace('/edit#gid=', '/export?format=csv&gid=')\n",
        "\n",
        "df=pd.read_csv(url_1)"
      ],
      "metadata": {
        "id": "m8MSBqOXDmAI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
